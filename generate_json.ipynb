{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/home/ubuntu/kimin/atlas_2022/ATLAS_pipline/data'\n",
    "\n",
    "task_name = 'Task500_ATLAS'\n",
    "target_base = os.path.join(base, task_name)\n",
    "imagesTr_path = os.path.join(target_base, \"imagesTr\")\n",
    "labelsTr_path = os.path.join(target_base, \"labelsTr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 데이터의 총합:  655\n",
      "prefix 별 train 개수:  [38, 12, 15, 37, 18, 111, 26, 29, 5, 12, 8, 6, 7, 8, 11, 16, 13, 5, 37, 24, 8, 49, 2, 45, 18, 2, 2, 7, 25, 23, 12, 7, 17]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "train_pathes = sorted(glob.glob(os.path.join(imagesTr_path, '*.nii.gz')))\n",
    "\n",
    "prefix_list = []\n",
    "for train_path in train_pathes:\n",
    "    prefix = train_path.split('/')[-1][:6]\n",
    "    prefix_list.append(prefix)\n",
    "    \n",
    "cnt_prefix_list = Counter(prefix_list)     \n",
    "train_cnt_list = list(cnt_prefix_list.values())\n",
    "print(\"train 데이터의 총합: \", sum(train_cnt_list))\n",
    "print(\"prefix 별 train 개수: \", train_cnt_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid 데이터의 총합:  60\n",
      "prefix 별 valid 개수:  [3, 1, 1, 3, 1, 11, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 4, 1, 4, 1, 1, 1, 1, 2, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "val_cnt_list = []\n",
    "for train_cnt in train_cnt_list:\n",
    "    val_ratio = train_cnt // 10 \n",
    "    if val_ratio == 0:\n",
    "        val_cnt_list.append(1)\n",
    "    else:\n",
    "        val_cnt_list.append(val_ratio)\n",
    "\n",
    "print(\"valid 데이터의 총합: \", sum(val_cnt_list))\n",
    "print(\"prefix 별 valid 개수: \", val_cnt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_train_val(count, val_cnt):\n",
    "    if count <= val_cnt:\n",
    "        return 'valid'\n",
    "    return 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pathes = sorted(glob.glob(os.path.join(imagesTr_path, '*.nii.gz')))\n",
    "\n",
    "target_imagesTr = []\n",
    "target_imagesVal = []\n",
    "count = 0\n",
    "total_train_cnt = 0\n",
    "current_idx = 0\n",
    "for train_path  in train_pathes:\n",
    "    name = train_path.split('/')[-1][:-7]\n",
    "    prefix_idx = int(train_path.split('/')[-1][:6][-1]) - 1\n",
    "    count += 1\n",
    "\n",
    "    flag = flag_train_val(count, val_cnt_list[prefix_idx])\n",
    "    \n",
    "    if flag == 'train':\n",
    "        target_imagesTr.append(name)\n",
    "    else:\n",
    "        target_imagesVal.append(name)\n",
    "    \n",
    "    if current_idx != prefix_idx:\n",
    "        count = 0\n",
    "        current_idx = prefix_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imagesTr 개수:  595\n",
      "imagesVal 개수:  60\n",
      "총합:  655\n"
     ]
    }
   ],
   "source": [
    "print(\"imagesTr 개수: \", len(target_imagesTr))\n",
    "print(\"imagesVal 개수: \", len(target_imagesVal))\n",
    "print(\"총합: \", len(target_imagesTr) + len(target_imagesVal) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "from typing import Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subfiles(folder: str, join: bool = True, prefix: str = None, suffix: str = None, sort: bool = True) -> List[str]:\n",
    "    if join:\n",
    "        l = os.path.join\n",
    "    else:\n",
    "        l = lambda x, y: y\n",
    "    res = [l(folder, i) for i in os.listdir(folder) if os.path.isfile(os.path.join(folder, i))\n",
    "           and (prefix is None or i.startswith(prefix))\n",
    "           and (suffix is None or i.endswith(suffix))]\n",
    "    if sort:\n",
    "        res.sort()\n",
    "    return res\n",
    "\n",
    "def save_json(obj, file: str, indent: int = 4, sort_keys: bool = True) -> None:\n",
    "    with open(file, 'w') as f:\n",
    "        json.dump(obj, f, sort_keys=sort_keys, indent=indent)\n",
    "\n",
    "def get_identifiers_from_splitted_files(folder: str):\n",
    "    uniques = np.unique([i[:-7] for i in subfiles(folder, suffix='.nii.gz', join=False)]) # edit i[:-12] to i[:-7]\n",
    "    return uniques\n",
    "\n",
    "def generate_dataset_json(output_file: str, imagesTr_dir: List, imagesVal_dir: List, modalities: Tuple,\n",
    "                          labels: dict, dataset_name: str, sort_keys=True, license: str = \"hands off!\", dataset_description: str = \"\",\n",
    "                          dataset_reference=\"\", dataset_release='0.0'):\n",
    "    \"\"\"\n",
    "    :param output_file: This needs to be the full path to the dataset.json you intend to write, so\n",
    "    output_file='DATASET_PATH/dataset.json' where the folder DATASET_PATH points to is the one with the\n",
    "    imagesTr and labelsTr subfolders\n",
    "    :param imagesTr_dir: path to the imagesTr folder of that dataset\n",
    "    :param imagesTs_dir: path to the imagesTs folder of that dataset. Can be None\n",
    "    :param modalities: tuple of strings with modality names. must be in the same order as the images (first entry\n",
    "    corresponds to _0000.nii.gz, etc). Example: ('T1', 'T2', 'FLAIR').\n",
    "    :param labels: dict with int->str (key->value) mapping the label IDs to label names. Note that 0 is always\n",
    "    supposed to be background! Example: {0: 'background', 1: 'edema', 2: 'enhancing tumor'}\n",
    "    :param dataset_name: The name of the dataset. Can be anything you want\n",
    "    :param sort_keys: In order to sort or not, the keys in dataset.json\n",
    "    :param license:\n",
    "    :param dataset_description:\n",
    "    :param dataset_reference: website of the dataset, if available\n",
    "    :param dataset_release:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # train_identifiers = get_identifiers_from_splitted_files(imagesTr_dir)\n",
    "\n",
    "    # if imagesTs_dir is not None:\n",
    "    #     test_identifiers = get_identifiers_from_splitted_files(imagesTs_dir)\n",
    "    # else:\n",
    "    #     test_identifiers = []\n",
    "\n",
    "    json_dict = {}\n",
    "    json_dict['name'] = dataset_name\n",
    "    json_dict['description'] = dataset_description\n",
    "    json_dict['tensorImageSize'] = \"4D\"\n",
    "    json_dict['reference'] = dataset_reference\n",
    "    json_dict['licence'] = license\n",
    "    json_dict['release'] = dataset_release\n",
    "    json_dict['modality'] = {str(i): modalities[i] for i in range(len(modalities))}\n",
    "    json_dict['labels'] = {str(i): labels[i] for i in labels.keys()}\n",
    "\n",
    "    json_dict['numTraining'] = len(imagesTr_dir)\n",
    "    json_dict['numTest'] = len(imagesVal_dir)\n",
    "    json_dict['training'] = [\n",
    "        {'image': \"./imagesTr/%s.nii.gz\" % i, \"label\": \"./labelsTr/%s.nii.gz\" % i} for i\n",
    "        in\n",
    "        imagesTr_dir]\n",
    "    json_dict['validation'] = [\n",
    "        {'image': \"./imagesTr/%s.nii.gz\" % i, \"label\": \"./labelsTr/%s.nii.gz\" % i} for i\n",
    "        in\n",
    "        imagesVal_dir]\n",
    "\n",
    "    if not output_file.endswith(\"dataset.json\"):\n",
    "        print(\"WARNING: output file name is not dataset.json! This may be intentional or not. You decide. \"\n",
    "              \"Proceeding anyways...\")\n",
    "    save_json(json_dict, os.path.join(output_file), sort_keys=sort_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/home/ubuntu/kimin/atlas_2022/ATLAS_pipline/data'\n",
    "\n",
    "task_name = 'Task500_ATLAS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dataset_json(\n",
    "    output_file=os.path.join(target_base, 'dataset.json'),\n",
    "    imagesTr_dir=target_imagesTr,\n",
    "    imagesVal_dir=target_imagesVal,\n",
    "    modalities=('T1',),\n",
    "    labels={0: 'background', 1: 'Stroke Lesion'},\n",
    "    dataset_name=task_name,\n",
    "    license='hands off!'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "681b2fdd5b9722b1d8e44570903527aaa8371d8c7b61f3bedb6c5b29233e448c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('lung-tumor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
